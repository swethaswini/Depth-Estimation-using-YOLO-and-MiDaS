import cv2
import numpy as np
import time
import threading
from picamera2 import Picamera2
from ultralytics import YOLO
import RPi.GPIO as GPIO
from pynput import keyboard
import AiPhile

# === Constants ===
KNOWN_DISTANCE = 76.2; KNOWN_WIDTH = 14.3; WIDTH, HEIGHT = 640, 480
TILT_THRESHOLD = 0.1
TRIM_PERCENTAGE = 17
CAT_YELLOW = (0, 175, 255); CAT_ORANGE = (0, 100, 255); CAT_BLACK = (40, 40, 40)
MINING_CLASSES = {'truck', 'excavator', 'loader', 'bulldozer', 'bus', 'car'}

# === GPIO Setup ===
IN1, IN2, ENA = 17, 27, 18; IN3, IN4, ENB = 23, 24, 25
RED_LED_PIN, YELLOW_LED_PIN, GREEN_LED_PIN = 22, 5, 6
GPIO.setmode(GPIO.BCM); GPIO.setwarnings(False)
for pin in [IN1, IN2, ENA, IN3, IN4, ENB, RED_LED_PIN, YELLOW_LED_PIN, GREEN_LED_PIN]:
    GPIO.setup(pin, GPIO.OUT)
pwm_a = GPIO.PWM(ENA, 1000); pwm_b = GPIO.PWM(ENB, 1000)
pwm_a.start(0); pwm_b.start(0)

def set_led(red=False, yellow=False, green=False): GPIO.output(RED_LED_PIN, red); GPIO.output(YELLOW_LED_PIN, yellow); GPIO.output(GREEN_LED_PIN, green)
def stop(): [GPIO.output(pin, GPIO.LOW) for pin in [IN1, IN2, IN3, IN4]]; pwm_a.ChangeDutyCycle(0); pwm_b.ChangeDutyCycle(0)
def forward(): GPIO.output(IN1, GPIO.HIGH); GPIO.output(IN2, GPIO.LOW); GPIO.output(IN3, GPIO.HIGH); GPIO.output(IN4, GPIO.LOW); pwm_a.ChangeDutyCycle(70); pwm_b.ChangeDutyCycle(70)
def backward(): GPIO.output(IN1, GPIO.LOW); GPIO.output(IN2, GPIO.HIGH); GPIO.output(IN3, GPIO.LOW); GPIO.output(IN4, GPIO.HIGH); pwm_a.ChangeDutyCycle(70); pwm_b.ChangeDutyCycle(70)
def left(): GPIO.output(IN1, GPIO.LOW); GPIO.output(IN2, GPIO.HIGH); GPIO.output(IN3, GPIO.HIGH); GPIO.output(IN4, GPIO.LOW); pwm_a.ChangeDutyCycle(70); pwm_b.ChangeDutyCycle(70)
def right(): GPIO.output(IN1, GPIO.HIGH); GPIO.output(IN2, GPIO.LOW); GPIO.output(IN3, GPIO.LOW); GPIO.output(IN4, GPIO.HIGH); pwm_a.ChangeDutyCycle(70); pwm_b.ChangeDutyCycle(70)

def focal_length(measured_distance, real_width, width_in_rf_image): return (width_in_rf_image * measured_distance) / real_width
def distance_finder(focal_length, real_face_width, face_width_in_frame): return (real_face_width * focal_length) / face_width_in_frame

def generate_fast_depth(frame):
    h, w = frame.shape[:2]
    vertical_depth = np.linspace(10.0, 3.0, h, dtype=np.float32).reshape(-1, 1)
    center_w = w // 2
    dist_from_center = np.abs(np.arange(w) - center_w)
    normalized_dist = dist_from_center / center_w
    perspective_strength = 1.0
    horizontal_gradient = normalized_dist * perspective_strength
    vertical_map = np.tile(vertical_depth, (1, w))
    horizontal_map = np.tile(horizontal_gradient, (h, 1))
    combined_depth_map = vertical_map + horizontal_map
    return combined_depth_map

def analyze_tilt_simple(depth_map, box):
    x1, y1, x2, y2 = box
    roi = depth_map[max(y1, y2-10):y2+1, x1:x2+1]
    if roi.size < 2: return None
    mid_point = roi.shape[1] // 2
    left_half, right_half = roi[:, :mid_point], roi[:, mid_point:]
    if left_half.size == 0 or right_half.size == 0: return None
    left = np.mean(left_half)
    right = np.mean(right_half)
    diff = abs(left - right)
    trim_info = "LEFT edge closer (Trim RIGHT)" if left < right else "RIGHT edge closer (Trim LEFT)"
    return {'left': left, 'right': right, 'diff': diff, 'trim_info': trim_info, 'needs_trim': diff > TILT_THRESHOLD}

def show_info_box(img, obj_name, left, right, diff, trim_rate, tilt_status, trim_info=None):
    x0, y0, box_w, box_h = img.shape[1]-330, 10, 320, 215
    overlay = img.copy(); cv2.rectangle(overlay, (x0, y0), (x0+box_w, y0+box_h), CAT_BLACK, -1)
    cv2.addWeighted(overlay, 0.7, img, 0.3, 0, img)
    y = y0 + 30
    cv2.putText(img, f"Object: {obj_name}", (x0+15, y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, CAT_YELLOW, 2)
    left_text = f"{left:.5f}m" if isinstance(left, (int, float)) else left
    right_text = f"{right:.5f}m" if isinstance(right, (int, float)) else right
    diff_text = f"{diff:.5f}m" if isinstance(diff, (int, float)) else diff
    trim_text = f"{trim_rate}%" if isinstance(trim_rate, int) else trim_rate
    y += 30; cv2.putText(img, f"Left depth: {left_text}", (x0+15, y), 0, 0.6, (255,255,255), 1)
    y += 25; cv2.putText(img, f"Right depth: {right_text}", (x0+15, y), 0, 0.6, (255,255,255), 1)
    y += 25; cv2.putText(img, f"Depth Difference: {diff_text}", (x0+15, y), 0, 0.6, CAT_ORANGE, 1)
    y += 25; cv2.putText(img, f"Trim rate: {trim_text}", (x0+15, y), 0, 0.6, CAT_ORANGE, 1)
    if trim_info: y += 25; cv2.putText(img, f"Info: {trim_info}", (x0+15, y), 0, 0.6, CAT_ORANGE, 1)
    y += 25; cv2.putText(img, f"Live analysis: {tilt_status}", (x0+15, y), 0, 0.6, (0,255,0) if tilt_status == "OK" else (0,0,255), 2)

# --- Initialization ---
face_detector = cv2.CascadeClassifier("/home/pi/Distance_measurement_using_single_camera/Raspberry_pi/watch-cascade.xml")
ref_image = cv2.imread("Ref_image.png"); gray = cv2.cvtColor(ref_image, cv2.COLOR_BGR2GRAY)
ref_faces = face_detector.detectMultiScale(gray, 1.3, 5)
if len(ref_faces) == 0: print("No face found in reference image."); stop(); GPIO.cleanup(); exit()
ref_face_width = ref_faces[0][2]
focal_length_found = focal_length(KNOWN_DISTANCE, KNOWN_WIDTH, ref_face_width)
picam2 = Picamera2(); picam2.preview_configuration.main.size = (WIDTH, HEIGHT)
picam2.preview_configuration.controls.FrameRate = 15; picam2.configure("preview"); picam2.start(); time.sleep(2)
model = YOLO("yolov8s.pt"); model.fuse()
running = True

def on_press(key):
    global running
    try:
        if key.char == 'w': forward()
        elif key.char == 's': backward()
        elif key.char == 'a': left()
        elif key.char == 'd': right()
        elif key.char == 'x':
            stop()
            print("Motors stopped via 'x' key")
    except AttributeError:
        if key == keyboard.Key.esc:
            running = False
            return False

# --- Main Loop ---
print("Dual Mode Running: WASD control + Object Detection (FINAL VERSION)")
with keyboard.Listener(on_press=on_press) as listener:
    prev_time = time.time()
    while running:
        frame = picam2.capture_array()
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR); frame = cv2.flip(frame, 0)
        h, w = frame.shape[:2]; half = w // 2

        left_frame = frame[:, :half].copy(); gray = cv2.cvtColor(left_frame, cv2.COLOR_BGR2GRAY)
        faces = face_detector.detectMultiScale(gray, 1.3, 5)
        if len(faces) == 0: set_led(green=True)
        for (x, y, fw, fh) in faces:
            distance = distance_finder(focal_length_found, KNOWN_WIDTH, fw)
            if distance < 50: set_led(red=True); stop()
            elif 50 <= distance <= 100: set_led(yellow=True)
            else: set_led(green=True)
            cv2.rectangle(left_frame, (x, y), (x+fw, y+fh), CAT_ORANGE, 2)
            AiPhile.textBGoutline(left_frame, f"{distance:.1f} cm", (30, 80), scaling=0.6, text_color=CAT_YELLOW)

        right_frame = frame[:, half:].copy()
        results = model(right_frame, imgsz=320, verbose=False)[0]
        depth_map = generate_fast_depth(right_frame)

        largest_object = None; max_area = 0; all_detections = []
        for box in results.boxes:
            x1,y1,x2,y2 = map(int, box.xyxy[0]); area = (x2-x1)*(y2-y1)
            detection_data = {'box': (x1, y1, x2, y2), 'name': model.names[int(box.cls[0])], 'conf': float(box.conf[0])}
            all_detections.append(detection_data)
            if area > max_area: max_area = area; largest_object = detection_data

        for det in all_detections:
            box, class_name, conf = det['box'], det['name'], det['conf']
            color = CAT_YELLOW if class_name.lower() in MINING_CLASSES else (0, 255, 0)
            offset = 10
            cv2.rectangle(right_frame, (box[0], box[1]), (box[2], box[3]), color, 2)
            cv2.rectangle(right_frame, (box[0]-offset, box[1]-offset), (box[2]-offset, box[3]-offset), color, 1)
            cv2.line(right_frame, (box[0], box[1]), (box[0]-offset, box[1]-offset), color, 1); cv2.line(right_frame, (box[2], box[1]), (box[2]-offset, box[1]-offset), color, 1)
            cv2.line(right_frame, (box[0], box[2]), (box[0]-offset, box[2]-offset), color, 1); cv2.line(right_frame, (box[2], box[2]), (box[2]-offset, box[2]-offset), color, 1)
            label = f"{class_name} {conf*100:.1f}%"; cv2.putText(right_frame, label, (box[0], box[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, CAT_YELLOW, 2)

        if largest_object:
            tilt = analyze_tilt_simple(depth_map, largest_object['box'])
            if tilt:
                obj_name_disp, left_disp, right_disp, diff_disp = largest_object['name'], tilt['left'], tilt['right'], tilt['diff']
                trim_rate_disp, status_disp, info_disp = TRIM_PERCENTAGE, "TRIM" if tilt['needs_trim'] else "OK", tilt['trim_info']
            else:
                obj_name_disp = largest_object['name']
                left_disp, right_disp, diff_disp, trim_rate_disp, status_disp, info_disp = "--", "--", "--", "--", "ERROR", "Analysis Failed"
        else:
            obj_name_disp, left_disp, right_disp, diff_disp, trim_rate_disp, status_disp, info_disp = "None", "--", "--", "--", "--", "SEARCHING", "No object detected"

        show_info_box(right_frame, obj_name_disp, left_disp, right_disp, diff_disp, trim_rate_disp, status_disp, info_disp)

        combined = np.hstack((left_frame, right_frame))
        fps = 5 / (time.time() - prev_time); prev_time = time.time()
        cv2.putText(combined, f"FPS: {fps:.2f}", (10, 30), 0, 0.8, (0,255,0), 2)
        cv2.imshow("Dual View (Face & YOLO Mining)", combined)

        if cv2.waitKey(1) & 0xFF == ord('q'): running = False; break

    listener.join(); stop(); set_led(False, False, False); GPIO.cleanup(); cv2.destroyAllWindows()
